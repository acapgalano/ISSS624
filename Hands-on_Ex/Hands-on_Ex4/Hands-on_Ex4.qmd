---
title: "Hands-on Exercise 4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method"
editor: visual
format: html
execute:
  warning: false
---

# Overview

This is a hands-on exercise based on [Chapter 6](https://r4gdsa.netlify.app/chap06.html "R4GDSA:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method") of [R for Geospatial Data Science and Analytics](https://r4gdsa.netlify.app/) by Dr.Â Kam Tin Seong and is a requirement under the class ISS624: Geospatial Analytics and Applications.

In the last exercise, we formed clusters and observed different patterns in the variables which defined each cluster. However, do we know the relationship between these variables?

## The Analytical Question

How are prices determined? Hedonic pricing is a model that identifies price factors according to the premise that price is determined both by internal characteristics of the good being sold and external factors affecting it. This is often used in the field of real estate to estimate property values. In this exercise, we determine to what extent certain structural and locational variables affected the resale prices of condominiums in 2015.

## The Main Concept: Geographically Weighted Regression (GWR)

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this exercise, we use GWR methods to build hedonic pricing models.

# Getting Started

## Loading the Packages

The code chunk loads the necessary packages for the exercise.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

::: {.callout-note icon="false"}
## ðŸŽ® LEVEL UP!

**NEW PACKAGES UNLOCKED: `olsrr`, `GWmodel`, `gtsummary`**

-   [**`olsrr`**](https://olsrr.rsquaredacademy.com/ "olsrr") - used for building OLS regression models

-   [**`GWmodel`**](https://cran.r-project.org/web/packages/GWmodel/ "CRAN: GWmodel") - stands for "geographically weighted models"; used for calibrating geographical weighted family of models

-   [**`gtsummary`**](https://www.danieldsjoberg.com/gtsummary/ "gtsummary") - used to create elegant and flexible publication-ready analytical and summary tables
:::

# Geospatial Data Wrangling

## Importing the Geospatial Data

The geospatial data used in this hands-on exercise is called '*MP14_SUBZONE_WEB_PL*' which is in ESRI shapefile format. It defines the URA Master Plan 2014's planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in the 'SVY21' projected coordinates system.

The code chunk below is used to import '*MP_SUBZONE_WEB_PL'* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

## Updating CRS Information

Since the simple feature object '*mpsz*' does not have EPSG information, the code chunk below updates the newly imported '*mpsz'* with the correct ESPG code (i.e.Â 3414).

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

```{r}
#| output: false
#| echo: false
st_make_valid(mpsz_svy21)
```

The code chunk below uses `st_crs()` to verify the newly transformed '*mpsz_svy21'* has EPSG set to 3414.

```{r}
st_crs(mpsz_svy21)
```

Next, we see the extent of '*mpsz_svy21'* using the `st_bbox()` of **`sf`** package.

```{r}
st_bbox(mpsz_svy21)
```

# Aspatial Data Wrangling

## Importing the Aspatial Data

The '*condo_resale_2015'* is in csv file format. The codes chunk below uses `read_csv()` function of **readr** package to import'*condo_resale_2015'* into R as a tibble data frame called '*condo_resale'*.

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

The code chunk below uses `glimpse()` to view the data structure of the columns.

```{r}
glimpse(condo_resale)
```

The code chunk below looks at the data in the `'XCOORD'` column.

```{r}
head(condo_resale$LONGITUDE) 
```

The code chunk below looks at the data in the `'YCOORD'` column.

```{r}
head(condo_resale$LATITUDE) 
```

Next, the function `summary()` is used to display the summary statistics of '*cond_resale'* tibble data frame.

```{r}
summary(condo_resale)
```

## Converting Tibble to Simple Feature Object

The code chunk below uses the function `st_as_sf()` to convert our tibble data frame to a simple feature data frame. We also use `st_transform()` once again to convert the coordinates WGS84 to SVY21 (which is the projected CRS of our geospatial data).

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
head(condo_resale.sf)
```

We now have a POINT feature data frame!

# Exploratory Data Analysis (EDA)

## Statistical Graphics

```{r}
#| fig-width: 12
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")
```

::: {.callout-note icon="false"}
## ðŸ”Ž OBSERVATION!

The figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.
:::

Since distribution for `'SELLING_PRICE'` is skewed, we need to normalize it. In this case we use **log transformation**. The code chunk below uses `mutate()` to apply the `log()` function to the `'SELLING_PRICE'` column.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

```{r}
#| fig-width: 12
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")
```

::: {.callout-note icon="false"}
## ðŸŒ¸ NEW OBSERVATION!

Visually, we can clearly see the distribution has moved towards the center and is closer to looking like a normal distribution.
:::

## Multiple Histogram Plots Distribution of Variables

```{r}
#| fig-width: 12
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="#e3879e")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="#e3879e")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

## Drawing Statistical Point Map

Lastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared using the **`tmap`** package.

First, we will turn on the interactive mode of tmap by setting `tmap_mode()` to "view".

```{r}
tmap_mode("view")
```

Next, the code chunks below is used to create an interactive point symbol map.

```{r}
#| output: false
#| echo: false
tmap_options(check.and.fix = TRUE)
```

```{r}
#| fig-width: 12
tm_shape(mpsz_svy21)+
  tm_polygons() +
  tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style ="quantile",
          palette = "RdPu") +
  tm_view(set.zoom.limits = c(11,14))
```

The dots shown in the map above represent the condos.

Now we need to set `tmap_mode()` back to "plot" for future use.

```{r}
tmap_mode("plot")
```

# Hedonic Pricing Modelling in R

## Simple Linear Regression Method

First, we build a simple linear regression model by using `'SELLING_PRICE'` as the dependent variable and `'AREA_SQM'` as the independent variable. The code chunk below uses `lm()` to fit the linear model.

```{r}
condo.slr <- lm(formula = SELLING_PRICE ~ AREA_SQM,
                data = condo_resale.sf)
```

The code chunk below uses `summary()` to view information on the model.

```{r}
summary(condo.slr)
```

The output report reveals that the `'SELLING_PRICE'` can be explained by using the formula:

$$ y = -258131.1 + 14719x_1$$

The $R^2$ of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.

Since p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of `'SELLING_PRICE'`. This will allow us to infer that simple linear regression model above is a good estimator of `'SELLING_PRICE'`.

To visualize the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot's geometry as shown in the code chunk below.

```{r}
#| fig-width: 12
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
    geom_point(col = "#cb6a82") +
    geom_smooth(method = lm, col = "#704276")
```

The figure above reveals that there are a few statistical outliers with relatively high selling prices.

## Multiple Linear Regression Method

Before building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other.

Correlation matrix is commonly used to visualize the relationships between the independent variables. Beside the `pairs()` of R, there are many packages support the display of a correlation matrix. In this section, the **`corrplot`** package will be used.

The code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in '*condo_resale'* data frame.

```{r}
#| fig-width: 12
#| fig-height: 12
corrplot(cor(condo_resale[, 5:23]), 
         diag = FALSE, order = "AOE",
         tl.pos = "td", 
         tl.cex = 0.5, 
         method = "number", 
         type = "upper")
```

Matrix reorder is very important for mining the hidden structure and patterns in the matrix. There are four methods in **`corrplot`**(parameter order), named "AOE", "FPC", "hclust", "alphabet"). In the code chunk above, AOE order is used. It orders the variables by using the ***angular order** of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

From the scatterplot matrix, it is clear that 'Freehold*'* is highly correlated to 'LEASE_99YEAR'. In line with this, it is wiser to only include either one of them in the subsequent model building. As a result, 'LEASE_99YEAR' is excluded in the subsequent model building.

## Hedonic Pricing Model Using Multiple Linear Regression Method

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM +
                  AGE    + 
                  PROX_CBD + PROX_CHILDCARE +
                  PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA +
                  PROX_HAWKER_MARKET + 
                  PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK +
                  PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH +
                  PROX_SHOPPING_MALL +
                  PROX_SUPERMARKET + 
                  PROX_BUS_STOP + 
                  NO_Of_UNITS +
                  FAMILY_FRIENDLY + 
                  FREEHOLD, 
                data=condo_resale.sf)

summary(condo.mlr)
```

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.

### Preparing Publication Quality Table

The code chunk below uses `ols_regress()` to create a more visually appealing and readable summary of the model.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM +
                   AGE + 
                   PROX_CBD + PROX_CHILDCARE +
                   PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  +
                   PROX_PARK + 
                   PROX_PRIMARY_SCH +
                   PROX_SHOPPING_MALL    +
                   PROX_BUS_STOP + 
                   NO_Of_UNITS + 
                   FAMILY_FRIENDLY +
                   FREEHOLD,
                 data=condo_resale.sf)

ols_regress(condo.mlr1)
```

The code chunk below uses `tbl_regression()` to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

### Checking for Multicolinearity

In the code chunk below, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) of **`olsrr`** package is used to test if there are sign of multicollinearity.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

### Test for Non-Linearity

In the code chunk below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of **`olsrr`** package is used to perform linearity assumption test.

```{r}
#| fig-width: 12
ols_plot_resid_fit(condo.mlr1)
```

The figure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.

### Test for Normality Assumption

Lastly, the code chunk below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of **`olsrr`** package to perform normality assumption test.

```{r}
#| fig-width: 12
ols_plot_resid_hist(condo.mlr1)
```

The figure reveals that the residual of the multiple linear regression model (i.e.Â condo.mlr1) is resemble normal distribution.

If you prefer formal statistical test methods, the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of **`olsrr`** package can be used as shown in the code chunk below.

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residuals are not normally distributed.

### Testing for Spatial Autocorrelation

The hedonic model is using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert ''condo_resale.sf' from a simple features data frame to a **SpatialPointsDataFrame**.

First, we will export the residual of the hedonic pricing model and save it as a data frame and join the newly created data frame with the 'condo_resales.sf' object.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)

condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, we will convert 'condo_resale.res.sf' from a simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)

condo_resale.sp
```

Now we can view the residuals mapped using **`tmap`** .

```{r}
#| fig-width: 12
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile",
          palette = "RdPu") +
  tm_view(set.zoom.limits = c(11,14))
```

```{r}
#| echo: false
#| output: false
tmap_mode("plot")
```

The figure above reveals that there is sign of spatial autocorrelation.

To prove that our observation is indeed true, the Moran's I test will be performed. To do that we need to create our distance-based weight matrix using `dnearneigh()`.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 
                 0, 
                 1500, 
                 longlat = FALSE)
                # longlat is FALSE cause XY coords

summary(nb)
```

Next, `nb2listw()` will be used to convert the output neighbours lists into a spatial weights.

```{r}

nb_lw <- nb2listw(nb, style = 'W')

summary(nb_lw)
```

Finally we do the Moran's I test using `lm.morantest()` for residual spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

## Building Hedonic Pricing Models using GWmodel

### Building Fixed Bandwidth GWR Model

### Computing fixed bandwidth

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + 
                     AGE + PROX_CBD + 
                     PROX_CHILDCARE +
                     PROX_ELDERLYCARE  +
                     PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + 
                     PROX_PARK + 
                     PROX_PRIMARY_SCH +
                     PROX_SHOPPING_MALL +
                     PROX_BUS_STOP + 
                     NO_Of_UNITS + 
                     FAMILY_FRIENDLY + 
                     FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The result shows that the recommended bandwidth is **971.3398 meters**. We use meters because that is the unit of measurement of our projected coordinate system.

#### GWModel method - fixed bandwidth

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + 
                         AGE + 
                         PROX_CBD + 
                         PROX_CHILDCARE + 
                         PROX_ELDERLYCARE  +
                         PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + 
                         PROX_PARK + 
                         PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + 
                         PROX_BUS_STOP + 
                         NO_Of_UNITS + 
                         FAMILY_FRIENDLY + 
                         FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved in a list of class "gwrm". The code below can be used to display the model output.

```{r}
gwr.fixed
```

The report shows that the adjusted $R^2$ of the gwr is **0.8430** which is significantly better than the global multiple linear regression model of **0.6472**.

### Building Adaptive Bandwidth GWR Model

Similar to the earlier section, used `bw.ger()` to determine the recommended data point to use.

The code chunk below look very similar to the one used to compute the fixed bandwidth except the `'adaptive'` argument has changed to "**TRUE"**.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + 
                        AGE  + 
                        PROX_CBD + 
                        PROX_CHILDCARE +
                        PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA +
                        PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH +
                        PROX_SHOPPING_MALL   +
                        PROX_BUS_STOP + 
                        NO_Of_UNITS + 
                        FAMILY_FRIENDLY + 
                        FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

The result shows that the 30 is the recommended data points to be used.

#### Constructing the adaptive bandwidth gwr model

The code chunk below calibrates the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel.

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + 
                            AGE + 
                            PROX_CBD + 
                            PROX_CHILDCARE +
                            PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA +
                            PROX_MRT + 
                            PROX_PARK + 
                            PROX_PRIMARY_SCH +
                            PROX_SHOPPING_MALL + 
                            PROX_BUS_STOP + 
                            NO_Of_UNITS +
                            FAMILY_FRIENDLY +
                            FREEHOLD, 
                          data=condo_resale.sp, 
                          bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)

gwr.adaptive
```

The report shows that the adjusted $R^2$ of the gwr is **0.8561** which is significantly better than the global multiple linear regression model of **0.6472**.

### Visualizing GWR Output

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)

condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)

condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)

condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))

glimpse(condo_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

## Visualizing Local R2

The code chunks below is used to create an interactive point symbol map.

```{r}
#| fig-width: 12
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1,
          palette = "RdPu") +
  tm_view(set.zoom.limits = c(11,14))
```

### By URA Planning Region

The code chunk below changes the boundaries or shapes to only those in the "CENTRAL REGION".

```{r}
#| fig-width: 12
tmap_mode("plot")
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
  tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1,
           palette = "RdPu")
```
